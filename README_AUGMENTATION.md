# Gu√≠a de Data Augmentation

Este notebook soporta dos m√©todos de data augmentation para balancear las clases minoritarias:

## üìã M√©todos Disponibles

### 1. EDA (Easy Data Augmentation) ‚ö° RECOMENDADO

**Ventajas:**
- ‚úÖ Muy r√°pido (~5-10 minutos para 50K+ muestras)
- ‚úÖ No requiere GPU
- ‚úÖ Bajo uso de memoria
- ‚úÖ Resultados efectivos para espa√±ol

**Desventajas:**
- ‚ö†Ô∏è Menor diversidad que back-translation
- ‚ö†Ô∏è Requiere diccionario de sin√≥nimos

**T√©cnicas aplicadas:**
- **Synonym Replacement**: Reemplaza palabras con sin√≥nimos (15% de palabras)
- **Random Insertion**: Inserta palabras aleatorias (15% de palabras)
- **Random Swap**: Intercambia posiciones de palabras (15% de palabras)
- **Random Deletion**: Elimina palabras aleatoriamente (10% probabilidad)

**Uso:**
```python
AUGMENTATION_METHOD = 'eda'  # En la celda de configuraci√≥n
```

---

### 2. Back-Translation üåç (M√°s preciso pero lento)

**Ventajas:**
- ‚úÖ Mayor diversidad sem√°ntica
- ‚úÖ Preserva mejor el significado original
- ‚úÖ Soporta m√∫ltiples idiomas intermedios

**Desventajas:**
- ‚ö†Ô∏è Muy lento (~2-4 horas en CPU para 50K+ muestras)
- ‚ö†Ô∏è Requiere modelos de traducci√≥n pesados (~2-3 GB de memoria)
- ‚ö†Ô∏è Necesita dependencias adicionales (torch, transformers)

**Proceso:**
1. Espa√±ol ‚Üí Ingl√©s/Franc√©s/Alem√°n
2. Idioma intermedio ‚Üí Espa√±ol
3. Resultado: Variaci√≥n sem√°ntica del texto original

**Uso:**
```python
AUGMENTATION_METHOD = 'backtranslation'  # En la celda de configuraci√≥n
```

**Dependencias adicionales:**
Descomenta en `requirements.txt`:
```bash
torch>=2.0.0
transformers>=4.21.0
sacremoses>=0.0.53
```

Luego instala:
```bash
pip install torch transformers sacremoses
```

---

## ‚öôÔ∏è Configuraci√≥n

### Configuraci√≥n de EDA

```python
EDA_CONFIG = {
    'alpha_sr': 0.15,  # Porcentaje para synonym replacement
    'alpha_ri': 0.15,  # Porcentaje para random insertion
    'alpha_rs': 0.15,  # Porcentaje para random swap
    'p_rd': 0.1        # Probabilidad de random deletion
}
```

**Recomendaciones:**
- Para textos cortos (<30 palabras): Reducir `alpha_*` a 0.10
- Para textos largos (>100 palabras): Aumentar `alpha_*` a 0.20
- `p_rd` siempre mantener bajo (0.05-0.15) para no perder informaci√≥n

### Configuraci√≥n de Back-Translation

```python
BACKTRANSLATION_CONFIG = {
    'languages': ['en', 'fr', 'de'],  # Idiomas intermedios
    'device': 'cpu'  # 'cuda' si tienes GPU disponible
}
```

**Idiomas soportados:**
- `'en'` - Ingl√©s (m√°s r√°pido, mejor calidad)
- `'fr'` - Franc√©s
- `'de'` - Alem√°n
- `'it'` - Italiano
- `'pt'` - Portugu√©s

**Nota:** Cada idioma adicional aumenta el tiempo de ejecuci√≥n. Para CPU, se recomienda usar solo `['en']`.

### Estrategia de Balanceo

```python
TARGET_SAMPLES = {
    0: 30000,  # Pol 1: 5K ‚Üí 30K (6x oversampling)
    1: 30000,  # Pol 2: 5K ‚Üí 30K (6x oversampling)
    2: 40000,  # Pol 3: 15K ‚Üí 40K (2.7x oversampling)
    3: 40000,  # Pol 4: 45K ‚Üí 40K (undersampling)
    4: 50000   # Pol 5: 136K ‚Üí 50K (undersampling)
}
```

**Personalizaci√≥n:**
- Ajusta los valores seg√∫n tu dataset
- Aumenta `TARGET_SAMPLES[0]` y `TARGET_SAMPLES[1]` si tienes muy pocas muestras en clases 1-2
- Reduce `TARGET_SAMPLES[4]` si quieres dataset m√°s peque√±o (m√°s r√°pido)

---

## üìä Comparaci√≥n de Rendimiento

| Caracter√≠stica | EDA | Back-Translation |
|---------------|-----|------------------|
| **Tiempo (50K muestras)** | 5-10 min | 2-4 horas |
| **Memoria RAM** | ~2 GB | ~8-12 GB |
| **GPU necesaria** | No | Recomendado |
| **Calidad sem√°ntica** | Buena | Excelente |
| **Diversidad** | Media | Alta |
| **Complejidad** | Baja | Alta |

---

## üöÄ Recomendaciones de Uso

### Caso 1: Desarrollo r√°pido (2-4 horas disponibles)
```python
AUGMENTATION_METHOD = 'eda'
EDA_CONFIG = {'alpha_sr': 0.15, 'alpha_ri': 0.15, 'alpha_rs': 0.15, 'p_rd': 0.1}
```

### Caso 2: M√°xima calidad (8+ horas disponibles, GPU disponible)
```python
AUGMENTATION_METHOD = 'backtranslation'
BACKTRANSLATION_CONFIG = {'languages': ['en', 'fr', 'de'], 'device': 'cuda'}
```

### Caso 3: Equilibrio (4-6 horas, CPU)
```python
AUGMENTATION_METHOD = 'backtranslation'
BACKTRANSLATION_CONFIG = {'languages': ['en'], 'device': 'cpu'}
```

### Caso 4: Dataset peque√±o (<10K muestras)
```python
AUGMENTATION_METHOD = 'backtranslation'
BACKTRANSLATION_CONFIG = {'languages': ['en'], 'device': 'cpu'}
TARGET_SAMPLES = {0: 5000, 1: 5000, 2: 8000, 3: 8000, 4: 10000}
```

---

## üìù Ejemplos de Salida

### EDA Original:
```
"La comida estuvo excelente y el servicio fue r√°pido"
```

### EDA Augmented:
```
"La comida estuvo genial y el servicio fue veloz"
"La comida r√°pido estuvo excelente y el servicio fue magn√≠fico"
```

### Back-Translation (ES‚ÜíEN‚ÜíES):
```
"La comida fue excelente y el servicio fue r√°pido"
```

---

## üêõ Troubleshooting

### Error: "No module named 'transformers'"
**Soluci√≥n:** Instala las dependencias de back-translation:
```bash
pip install torch transformers sacremoses
```

### Error: "CUDA out of memory"
**Soluci√≥n:** Cambia a CPU:
```python
BACKTRANSLATION_CONFIG = {'languages': ['en'], 'device': 'cpu'}
```

### Error: "KeyError en SYNONYMS"
**Soluci√≥n:** Expande el diccionario `SYNONYMS` en la celda 2.1 con m√°s palabras relevantes a tu dominio.

### Advertencia: "Back-translation muy lento"
**Soluci√≥n:** 
1. Reduce idiomas: `['en']` en vez de `['en', 'fr', 'de']`
2. Usa EDA en su lugar
3. Reduce `TARGET_SAMPLES`

---

## üìö Referencias

- **EDA Paper**: [Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks](https://arxiv.org/abs/1901.11196)
- **Back-Translation**: [Understanding Back-Translation at Scale](https://arxiv.org/abs/1808.09381)
- **Helsinki NLP Models**: [OPUS-MT Translation Models](https://huggingface.co/Helsinki-NLP)

---

## ‚úÖ Checklist Pre-Ejecuci√≥n

- [ ] Decidir m√©todo: `'eda'` (r√°pido) o `'backtranslation'` (preciso)
- [ ] Configurar `AUGMENTATION_METHOD` en celda 1.5
- [ ] Ajustar par√°metros en `EDA_CONFIG` o `BACKTRANSLATION_CONFIG`
- [ ] Verificar `TARGET_SAMPLES` seg√∫n necesidades
- [ ] Si usas back-translation: Instalar torch/transformers
- [ ] Estimar tiempo: EDA ~10 min, Back-translation ~2-4 hrs
- [ ] Verificar espacio en disco: ~500 MB para archivos de salida

---

## üí° Tip Final

Para la mayor√≠a de casos, **EDA es suficiente y mucho m√°s eficiente**. Solo usa back-translation si:
- Tienes GPU disponible
- Necesitas m√°xima calidad sem√°ntica
- Tienes tiempo suficiente (>4 horas)
- El dataset es peque√±o (<20K muestras a augmentar)
